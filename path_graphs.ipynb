{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k4g4e4zYhvpn"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import vmap, jit\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d3YSizSS_iTe"
      },
      "outputs": [],
      "source": [
        "import model\n",
        "import train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEc_D7tl_Huz"
      },
      "source": [
        "# Copy Task\n",
        "\n",
        "## Problem\n",
        "\n",
        "Given a repeating sequence of distinct tokens, continue the pattern. This equates to learning an induction head.\n",
        "\n",
        "## Dataset Generation\n",
        "\n",
        "The dataset consists of sequences of varying length that contain a repeating pattern and cut-off abruptly. The goal is to continue the sequence correctly. There is no semantic meaning behind tokens, so they can be randomly generated at init and frozen.\n",
        "\n",
        "E.g. abcabcabca should be continued with bcabcabc\n",
        "\n",
        "### Base case\n",
        "\n",
        "The most simple case will simply have 64-character strings containing repeating character sequences of 4 to 15 distinct characters, so we see 8 to 4 repetitions. To start we can use 32 distinct tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc3Zj_wEA4T8",
        "outputId": "302e1bb8-74de-4a2d-e41a-fc163838d6fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset copytask already exists\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(64, 1572864, 98304)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create dataset; we need to add masking on the loss function!\n",
        "key_d1 = jax.random.PRNGKey(0)\n",
        "\n",
        "dataset_name = 'copytask'\n",
        "token_arr = jnp.arange(32, dtype=jnp.uint16)\n",
        "sample_len = 64\n",
        "assert sample_len >= len(token_arr)\n",
        "n_data = 2**19*3\n",
        "patt_min = 4\n",
        "patt_max = 15\n",
        "assert patt_max >= patt_min\n",
        "assert n_data % (patt_max-patt_min+1) == 0\n",
        "\n",
        "pattern_inds = []\n",
        "for patt_len in range(patt_min, patt_max+1):\n",
        "  p = jnp.tile(jnp.arange(patt_len), 1+sample_len//patt_len)[:sample_len]\n",
        "  pattern_inds.append(p)\n",
        "pattern_inds = jnp.array(pattern_inds)\n",
        "\n",
        "key_gen, key_shuffle = jax.random.split(key_d1)\n",
        "key_perms = jax.random.split(key_gen, n_data)\n",
        "\n",
        "tok_permutations = vmap(lambda k : jax.random.permutation(k, token_arr))(key_perms)\n",
        "# tok_permutations = jax.random.choice(key_gen, token_arr, (n_data, sample_len))\n",
        "pattern_inds_expanded = jnp.tile(pattern_inds, (n_data//len(pattern_inds), 1))\n",
        "\n",
        "data = vmap(lambda i : tok_permutations[i][pattern_inds_expanded[i]])(jnp.arange(n_data))\n",
        "data = data[jax.random.permutation(key_shuffle, jnp.arange(n_data))] # shuffle the data\n",
        "\n",
        "val_data_len = n_data//16\n",
        "train_ids = np.array(data, dtype=np.uint16)[:-val_data_len].flatten()\n",
        "val_ids = np.array(data, dtype=np.uint16)[-val_data_len:].flatten()\n",
        "try:\n",
        "  os.mkdir(dataset_name)\n",
        "except:\n",
        "  print(f'dataset {dataset_name} already exists')\n",
        "train_ids.tofile(os.path.join(dataset_name, 'train.bin'))\n",
        "val_ids.tofile(os.path.join(dataset_name, 'val.bin'))\n",
        "\n",
        "sample_len, n_data, val_data_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h4qUaiwA1L1k"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "Yff_V_BvpXzy",
        "outputId": "a6536ddb-6dc9-4b44-ecc7-fa408df64f29"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "use_masked_loss=True requires data with masks (filenames containing 'mask'), but input paths are 'copytask/val.bin' and 'copytask/val.bin'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%capture\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/val.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_val_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/val.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43membd_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_layer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# should match the task sequence length so tasks are independently trained on\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_iters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# val_data_len // 64, # number of examples // batch_size\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_iters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_decay_iters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken_arr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_masked_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_interval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# rope_base = 10*sample_len,\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m display(config)\n",
            "File \u001b[0;32m<string>:40\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, mesh_axis_names, mesh_shape, input_bin, input_val_bin, wandb_project, wandb_entity, wandb_run_name, wandb_group, wandb_job_type, wandb_tags, wandb_notes, wandb_mode, wandb_log_code, max_iters, warmup_iters, lr_decay_iters, eval_interval, eval_iters, log_interval, save_every, batch_size, gradient_accumulation_steps, learning_rate, min_lr, weight_decay, beta1, beta2, grad_clip, n_layer, embd_dim, head_dim, block_size, vocab_size, dropout, rope_base, seed, use_masked_loss)\u001b[0m\n",
            "File \u001b[0;32m~/modded-nanogpt-jax/train.py:271\u001b[0m, in \u001b[0;36mTrainConfig.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmesh_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, (jax\u001b[38;5;241m.\u001b[39mdevice_count(),))\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_masked_loss:\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_has_masks, (\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_masked_loss=True requires data with masks (filenames containing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut input paths are \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_bin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_val_bin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     )\n",
            "\u001b[0;31mAssertionError\u001b[0m: use_masked_loss=True requires data with masks (filenames containing 'mask'), but input paths are 'copytask/val.bin' and 'copytask/val.bin'"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "config = train.TrainConfig(\n",
        "    input_bin=f\"{dataset_name}/val.bin\",\n",
        "    input_val_bin=f\"{dataset_name}/val.bin\",\n",
        "    embd_dim = 128,\n",
        "    head_dim = 128,\n",
        "    n_layer = 2,\n",
        "    block_size = sample_len, # should match the task sequence length so tasks are independently trained on\n",
        "    batch_size = 64,\n",
        "    gradient_accumulation_steps = 1,\n",
        "    max_iters = 10_000,\n",
        "    eval_iters = 10, # val_data_len // 64, # number of examples // batch_size\n",
        "    learning_rate = 3e-3,\n",
        "    warmup_iters = 100,\n",
        "    lr_decay_iters = 10_000,\n",
        "    vocab_size = len(token_arr),\n",
        "    use_masked_loss = True,\n",
        "    \n",
        "    log_interval = 500,\n",
        "    eval_interval = 1_000,\n",
        "    # rope_base = 10*sample_len,\n",
        "\n",
        ")\n",
        "display(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zQZ9oUxp9F4C",
        "outputId": "530bd668-804e-4a81-f420-af17601e5e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[wandb] No credentials found. Falling back to offline mode.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/home/nikola.georgiev2000/modded-nanogpt-jax/wandb/offline-run-20251223_200229-osc6wywd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Number of parameters: 0.40M\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Loading training data...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Process 0/1 prepared dataset from 1 file(s): 6,291,456 tokens, 0.01 GB on disk.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Process 0/1 prepared loader with 10000 batches.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Loaded 10000 training batches.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Loading validation data...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Process 0/1 prepared dataset from 1 file(s): 6,291,456 tokens, 0.01 GB on disk.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Process 0/1 prepared loader with 10 batches.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Loaded 10 validation batches.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting Ahead-of-Time (AOT) compilation...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 0.40M\n",
            "Loading training data...\n",
            "Process 0/1 prepared dataset from 1 file(s): 6,291,456 tokens, 0.01 GB on disk.\n",
            "Process 0/1 prepared loader with 10000 batches.\n",
            "Loaded 10000 training batches.\n",
            "Loading validation data...\n",
            "Process 0/1 prepared dataset from 1 file(s): 6,291,456 tokens, 0.01 GB on disk.\n",
            "Process 0/1 prepared loader with 10 batches.\n",
            "Loaded 10 validation batches.\n",
            "Starting Ahead-of-Time (AOT) compilation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: AOT compilation finished.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting training...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 0...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AOT compilation finished.\n",
            "Starting training...\n",
            "Running validation for step 0...\n",
            "model/total_params: 397696 | model/attn_params: 131072 | model/mlp_params: 261888 | model/embed_params: 4096 | model/vocab_size: 32 | val_loss: 3.646 | step: 0 | lr: 0 | loss: 3.646\n",
            "step: 500 | lr: 0.002988 | loss: 0.6871\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 1000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 1000...\n",
            "val_loss: 0.6234 | step: 1000 | lr: 0.00294 | loss: 0.592\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Cycling dataset...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 1500 | lr: 0.002856 | loss: 0.5835\n",
            "Cycling dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 2000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 2000...\n",
            "val_loss: 0.5898 | step: 2000 | lr: 0.002738 | loss: 0.6058\n",
            "step: 2500 | lr: 0.00259 | loss: 0.5522\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 3000...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Cycling dataset...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 3000...\n",
            "val_loss: 0.5789 | step: 3000 | lr: 0.002414 | loss: 0.6009\n",
            "Cycling dataset...\n",
            "step: 3500 | lr: 0.002216 | loss: 0.6025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 4000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 4000...\n",
            "val_loss: 0.5659 | step: 4000 | lr: 0.002001 | loss: 0.588\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Cycling dataset...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step: 4500 | lr: 0.001773 | loss: 0.5663\n",
            "Cycling dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 5000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 5000...\n",
            "val_loss: 0.5598 | step: 5000 | lr: 0.001539 | loss: 0.528\n",
            "step: 5500 | lr: 0.001304 | loss: 0.5465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 6000...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Cycling dataset...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 6000...\n",
            "val_loss: 0.5507 | step: 6000 | lr: 0.001074 | loss: 0.4925\n",
            "Cycling dataset...\n",
            "step: 6500 | lr: 0.0008556 | loss: 0.5283\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 7000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 7000...\n",
            "val_loss: 0.5459 | step: 7000 | lr: 0.0006536 | loss: 0.571\n",
            "step: 7500 | lr: 0.0004733 | loss: 0.5358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Cycling dataset...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cycling dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 8000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 8000...\n",
            "val_loss: 0.5414 | step: 8000 | lr: 0.0003192 | loss: 0.5126\n",
            "step: 8500 | lr: 0.0001951 | loss: 0.5281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 9000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 9000...\n",
            "val_loss: 0.5409 | step: 9000 | lr: 0.0001041 | loss: 0.5269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Cycling dataset...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cycling dataset...\n",
            "step: 9500 | lr: 4.865e-05 | loss: 0.54\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Final validation...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 9999...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Training finished.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Saved checkpoint to logs/osc6wywd//state_step009999.pkl\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final validation...\n",
            "Running validation for step 9999...\n",
            "step: 9999 | val_loss: 0.5403\n",
            "Training finished.\n",
            "Saved checkpoint to logs/osc6wywd//state_step009999.pkl\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▁███▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁</td></tr><tr><td>model/attn_params</td><td>▁</td></tr><tr><td>model/embed_params</td><td>▁</td></tr><tr><td>model/mlp_params</td><td>▁</td></tr><tr><td>model/total_params</td><td>▁</td></tr><tr><td>model/vocab_size</td><td>▁</td></tr><tr><td>step</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.53996</td></tr><tr><td>lr</td><td>5e-05</td></tr><tr><td>model/attn_params</td><td>131072</td></tr><tr><td>model/embed_params</td><td>4096</td></tr><tr><td>model/mlp_params</td><td>261888</td></tr><tr><td>model/total_params</td><td>397696</td></tr><tr><td>model/vocab_size</td><td>32</td></tr><tr><td>step</td><td>9999</td></tr><tr><td>val_loss</td><td>0.54034</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "You can sync this run to the cloud by running:<br><code>wandb sync /home/nikola.georgiev2000/modded-nanogpt-jax/wandb/offline-run-20251223_200229-osc6wywd<code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/offline-run-20251223_200229-osc6wywd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "params = train.train_loop(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WnBGTyqv9TVx"
      },
      "outputs": [],
      "source": [
        "test_input = [1,2,3,4,5,6,7,8]*1\n",
        "\n",
        "start_len = len(test_input)\n",
        "for i in range(start_len, sample_len+1):\n",
        "  padded_test_input = test_input + [0]*(sample_len - len(test_input))\n",
        "  rope_params = model.precompute_rope(config.get_model_config(), None)\n",
        "  preds = model.gpt_forward(params, rope_params,jnp.array(test_input)[None,:], config.get_model_config())\n",
        "  new_ind = jnp.argmax(preds[0][i])\n",
        "  test_input.append(new_ind.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqsr1PG7-jCx",
        "outputId": "a368c05e-7698-4943-aa28-a452d80f8fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing input:\n",
            "[1, 2, 3, 4, 5, 6, 7, 8]\n",
            "Predicted output:\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 21]\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing input:\")\n",
        "print(test_input[:start_len])\n",
        "\n",
        "print(\"Predicted output:\")\n",
        "print(test_input[start_len:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX-Ei7j0tETJ"
      },
      "source": [
        "# Path Graph\n",
        "\n",
        "## Problem\n",
        "\n",
        "Given a 'goal' token, identify which unique 'path' in context it comes from and return all tokens in the path up to and including the goal.\n",
        "\n",
        "Concretely, suppose your context has two paths: A,B,C and P,Q,R,S. Given a goal R, we would return P,Q,R. This tests the model's ability to build pointers between tokens based on context.\n",
        "\n",
        "## Dataset Generation\n",
        "\n",
        "The dataset consists of a list of sequences. Sequences can be one of three types:\n",
        "- context\n",
        "- input\n",
        "- output\n",
        "\n",
        "No nesting of sequences is allowed.\n",
        "\n",
        "#### Tokenization\n",
        "\n",
        "Overall will use 128 tokens.\n",
        "\n",
        "Special Tokens (with index):\n",
        "- Context Start: 0\n",
        "- Input Start: 1\n",
        "- Output start: 2\n",
        "\n",
        "Each special token implicitly ends the previous sequence and starts a new one. These embeddings can be learned.\n",
        "\n",
        "All remaining tokens are exchangeable, i.e. only act as pointers and contain no semantic meaning. Their embeddings will be fixed and initialized randomly.\n",
        "\n",
        "1. Minimal example\n",
        "- 0, 3,4,5,6, 1, 5, 2, 3,4,5\n",
        "\n",
        "2. Two contexts\n",
        "- 0, 3,4,5,6, 0, 7,8,9, 1, 4, 2, 3,4\n",
        "\n",
        "3. Stream of problems (context grows and problems arrive independently)\n",
        "- 0, 3,4,5,6, 0, 7,8,9, 1, 5, 2, 3,4,5, 0, 10,11,12,13,14, 1, 4, 2, 3,4, 1, 12, 2, 10,11,12\n",
        "\n",
        "For position encoding we'll use RoPE which works well with QK-norm attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fjCOwdxKGfhS"
      },
      "outputs": [],
      "source": [
        "# TBD, need to add masking to loss_fn()!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
