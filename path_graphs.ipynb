{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k4g4e4zYhvpn"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import vmap, jit\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d3YSizSS_iTe"
      },
      "outputs": [],
      "source": [
        "import model\n",
        "import train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEc_D7tl_Huz"
      },
      "source": [
        "# Copy Task\n",
        "\n",
        "## Problem\n",
        "\n",
        "Given a repeating sequence of distinct tokens, continue the pattern. This equates to learning an induction head.\n",
        "\n",
        "## Dataset Generation\n",
        "\n",
        "The dataset consists of sequences of varying length that contain a repeating pattern and cut-off abruptly. The goal is to continue the sequence correctly. There is no semantic meaning behind tokens, so they can be randomly generated at init and frozen.\n",
        "\n",
        "E.g. abcabcabca should be continued with bcabcabc\n",
        "\n",
        "### Base case\n",
        "\n",
        "The most simple case will simply have 64-character strings containing repeating character sequences of 4 to 15 distinct characters, so we see 8 to 4 repetitions. To start we can use 32 distinct tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc3Zj_wEA4T8",
        "outputId": "302e1bb8-74de-4a2d-e41a-fc163838d6fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset copytask already exists\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(64, 1572864, 98304)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create dataset; we need to add masking on the loss function!\n",
        "key_d1 = jax.random.PRNGKey(0)\n",
        "\n",
        "dataset_name = 'copytask'\n",
        "token_arr = jnp.arange(32, dtype=jnp.uint16)\n",
        "sample_len = 64\n",
        "assert sample_len >= len(token_arr)\n",
        "n_data = 2**19*3\n",
        "patt_min = 4\n",
        "patt_max = 15\n",
        "assert patt_max >= patt_min\n",
        "assert n_data % (patt_max-patt_min+1) == 0\n",
        "\n",
        "pattern_inds = []\n",
        "for patt_len in range(patt_min, patt_max+1):\n",
        "  p = jnp.tile(jnp.arange(patt_len), 1+sample_len//patt_len)[:sample_len]\n",
        "  pattern_inds.append(p)\n",
        "pattern_inds = jnp.array(pattern_inds)\n",
        "\n",
        "key_gen, key_shuffle = jax.random.split(key_d1)\n",
        "key_perms = jax.random.split(key_gen, n_data)\n",
        "\n",
        "tok_permutations = vmap(lambda k : jax.random.permutation(k, token_arr))(key_perms)\n",
        "# tok_permutations = jax.random.choice(key_gen, token_arr, (n_data, sample_len))\n",
        "pattern_inds_expanded = jnp.tile(pattern_inds, (n_data//len(pattern_inds), 1))\n",
        "\n",
        "data = vmap(lambda i : tok_permutations[i][pattern_inds_expanded[i]])(jnp.arange(n_data))\n",
        "data = data[jax.random.permutation(key_shuffle, jnp.arange(n_data))] # shuffle the data\n",
        "\n",
        "val_data_len = n_data//16\n",
        "train_ids = np.array(data, dtype=np.uint16)[:-val_data_len].flatten()\n",
        "val_ids = np.array(data, dtype=np.uint16)[-val_data_len:].flatten()\n",
        "try:\n",
        "  os.mkdir(dataset_name)\n",
        "except:\n",
        "  print(f'dataset {dataset_name} already exists')\n",
        "train_ids.tofile(os.path.join(dataset_name, 'train.bin'))\n",
        "val_ids.tofile(os.path.join(dataset_name, 'val.bin'))\n",
        "\n",
        "sample_len, n_data, val_data_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h4qUaiwA1L1k"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "Yff_V_BvpXzy",
        "outputId": "a6536ddb-6dc9-4b44-ecc7-fa408df64f29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TrainConfig(mesh_axis_names=('dp',), mesh_shape=(4,), input_bin='copytask/val.bin', input_val_bin='copytask/val.bin', wandb_project='gpt-jax', wandb_entity=None, wandb_run_name=None, wandb_group=None, wandb_job_type=None, wandb_tags=(), wandb_notes=None, wandb_mode='online', wandb_log_code=True, max_iters=10000, warmup_iters=100, lr_decay_iters=10000, eval_interval=1000, eval_iters=10, log_interval=500, save_every=0, batch_size=64, gradient_accumulation_steps=1, learning_rate=0.003, min_lr=3e-05, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, n_layer=2, embd_dim=256, head_dim=256, block_size=64, vocab_size=32, dropout=0.0, rope_base=10000.0, seed=1337)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# %%capture\n",
        "config = train.TrainConfig(\n",
        "    input_bin=f\"{dataset_name}/val.bin\",\n",
        "    input_val_bin=f\"{dataset_name}/val.bin\",\n",
        "    embd_dim = 256,\n",
        "    head_dim = 256,\n",
        "    n_layer = 2,\n",
        "    block_size = sample_len, # should match the task sequence length so tasks are independently trained on\n",
        "    batch_size = 64,\n",
        "    gradient_accumulation_steps = 1,\n",
        "    max_iters = 10_000,\n",
        "    eval_iters = 10, # val_data_len // 64, # number of examples // batch_size\n",
        "    learning_rate = 3e-3,\n",
        "    warmup_iters = 100,\n",
        "    lr_decay_iters = 10_000,\n",
        "    vocab_size = len(token_arr),\n",
        "    \n",
        "    log_interval = 500,\n",
        "    eval_interval = 1_000,\n",
        "    # rope_base = 10*sample_len,\n",
        "\n",
        ")\n",
        "display(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zQZ9oUxp9F4C",
        "outputId": "530bd668-804e-4a81-f420-af17601e5e99"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "socket.send() raised exception.\n",
            "socket.send() raised exception.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[wandb] No credentials found. Falling back to offline mode.\n",
            "[wandb] init failed (BrokenPipeError: [Errno 32] Broken pipe). Disabling W&B.\n",
            "Number of parameters: 1.58M\n",
            "Starting Ahead-of-Time (AOT) compilation...\n",
            "AOT compilation finished.\n",
            "Loading training data...\n",
            "Process 0/1 prepared dataset from 1 file(s): 6,291,456 tokens, 0.01 GB on disk.\n",
            "Process 0/1 prepared loader with 10000 batches.\n",
            "Loaded 10000 training batches.\n",
            "Loading validation data...\n",
            "Process 0/1 prepared dataset from 1 file(s): 6,291,456 tokens, 0.01 GB on disk.\n",
            "Process 0/1 prepared loader with 10 batches.\n",
            "Loaded 10 validation batches.\n",
            "Starting training...\n",
            "Running validation for step 0...\n",
            "model/total_params: 1581312 | model/attn_params: 524288 | model/mlp_params: 1047552 | model/embed_params: 8192 | model/vocab_size: 32 | val_loss: 3.714 | step: 0 | lr: 0 | loss: 3.738\n",
            "step: 500 | lr: 0.002988 | loss: 1.33\n",
            "Running validation for step 1000...\n",
            "val_loss: 0.9369 | step: 1000 | lr: 0.00294 | loss: 0.8852\n",
            "step: 1500 | lr: 0.002856 | loss: 0.7923\n",
            "Cycling dataset...\n",
            "Running validation for step 2000...\n",
            "val_loss: 0.766 | step: 2000 | lr: 0.002738 | loss: 0.852\n",
            "step: 2500 | lr: 0.00259 | loss: 0.6614\n",
            "Running validation for step 3000...\n",
            "val_loss: 0.6577 | step: 3000 | lr: 0.002414 | loss: 0.756\n",
            "Cycling dataset...\n",
            "step: 3500 | lr: 0.002216 | loss: 0.7014\n",
            "Running validation for step 4000...\n",
            "val_loss: 0.6391 | step: 4000 | lr: 0.002001 | loss: 0.6424\n",
            "step: 4500 | lr: 0.001773 | loss: 0.5889\n",
            "Cycling dataset...\n",
            "Running validation for step 5000...\n",
            "val_loss: 0.586 | step: 5000 | lr: 0.001539 | loss: 0.5376\n",
            "step: 5500 | lr: 0.001304 | loss: 0.5582\n",
            "Running validation for step 6000...\n",
            "val_loss: 0.5524 | step: 6000 | lr: 0.001074 | loss: 0.4927\n",
            "Cycling dataset...\n",
            "step: 6500 | lr: 0.0008556 | loss: 0.5269\n",
            "Running validation for step 7000...\n",
            "val_loss: 0.541 | step: 7000 | lr: 0.0006536 | loss: 0.5664\n",
            "step: 7500 | lr: 0.0004733 | loss: 0.5297\n",
            "Cycling dataset...\n",
            "Running validation for step 8000...\n",
            "val_loss: 0.5347 | step: 8000 | lr: 0.0003192 | loss: 0.5069\n",
            "step: 8500 | lr: 0.0001951 | loss: 0.52\n",
            "Running validation for step 9000...\n",
            "val_loss: 0.533 | step: 9000 | lr: 0.0001041 | loss: 0.5196\n",
            "Cycling dataset...\n",
            "step: 9500 | lr: 4.865e-05 | loss: 0.5347\n",
            "Final validation...\n",
            "Running validation for step 9999...\n",
            "step: 9999 | val_loss: 0.5319\n",
            "Training finished.\n",
            "Saved checkpoint to logs/73a00d3e-7b31-4a0b-b0bf-04d966bdb241//state_step009999.pkl\n"
          ]
        }
      ],
      "source": [
        "params = train.train_loop(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WnBGTyqv9TVx"
      },
      "outputs": [],
      "source": [
        "test_input = [1,5,7,3,6]*1\n",
        "\n",
        "start_len = len(test_input)\n",
        "for i in range(start_len, sample_len+1):\n",
        "  padded_test_input = test_input + [0]*(sample_len - len(test_input))\n",
        "  rope_params = model.precompute_rope(config.get_model_config(), None)\n",
        "  preds = model.gpt_forward(params, rope_params,jnp.array(test_input)[None,:], config.get_model_config())\n",
        "  new_ind = jnp.argmax(preds[0][i])\n",
        "  test_input.append(new_ind.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqsr1PG7-jCx",
        "outputId": "a368c05e-7698-4943-aa28-a452d80f8fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing input:\n",
            "[1, 5, 7, 3, 6]\n",
            "Predicted output:\n",
            "[1, 5, 7, 3, 6, 1, 5, 7, 3, 6, 1, 5, 7, 3, 6, 1, 5, 7, 3, 6, 1, 5, 7, 3, 6, 1, 5, 7, 3, 6, 1, 5, 7, 3, 6, 1, 5, 7, 3, 6, 1, 5, 7, 3, 6, 1, 5, 7, 3, 6, 1, 5, 7, 3, 6, 1, 5, 7, 3, 23]\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing input:\")\n",
        "print(test_input[:start_len])\n",
        "\n",
        "print(\"Predicted output:\")\n",
        "print(test_input[start_len:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX-Ei7j0tETJ"
      },
      "source": [
        "# Path Graph\n",
        "\n",
        "## Problem\n",
        "\n",
        "Given a 'goal' token, identify which unique 'path' in context it comes from and return all tokens in the path up to and including the goal.\n",
        "\n",
        "Concretely, suppose your context has two paths: A,B,C and P,Q,R,S. Given a goal R, we would return P,Q,R. This tests the model's ability to build pointers between tokens based on context.\n",
        "\n",
        "## Dataset Generation\n",
        "\n",
        "The dataset consists of a list of sequences. Sequences can be one of three types:\n",
        "- context\n",
        "- input\n",
        "- output\n",
        "\n",
        "No nesting of sequences is allowed.\n",
        "\n",
        "#### Tokenization\n",
        "\n",
        "Overall will use 128 tokens.\n",
        "\n",
        "Special Tokens (with index):\n",
        "- Context Start: 0\n",
        "- Input Start: 1\n",
        "- Output start: 2\n",
        "\n",
        "Each special token implicitly ends the previous sequence and starts a new one. These embeddings can be learned.\n",
        "\n",
        "All remaining tokens are exchangeable, i.e. only act as pointers and contain no semantic meaning. Their embeddings will be fixed and initialized randomly.\n",
        "\n",
        "1. Minimal example\n",
        "- 0, 3,4,5,6, 1, 5, 2, 3,4,5\n",
        "\n",
        "2. Two contexts\n",
        "- 0, 3,4,5,6, 0, 7,8,9, 1, 4, 2, 3,4\n",
        "\n",
        "3. Stream of problems (context grows and problems arrive independently)\n",
        "- 0, 3,4,5,6, 0, 7,8,9, 1, 5, 2, 3,4,5, 0, 10,11,12,13,14, 1, 4, 2, 3,4, 1, 12, 2, 10,11,12\n",
        "\n",
        "For position encoding we'll use RoPE which works well with QK-norm attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fjCOwdxKGfhS"
      },
      "outputs": [],
      "source": [
        "# TBD, need to add masking to loss_fn()!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
