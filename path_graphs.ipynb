{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k4g4e4zYhvpn"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from jax import vmap, jit\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "d3YSizSS_iTe"
      },
      "outputs": [],
      "source": [
        "import model\n",
        "import train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEc_D7tl_Huz"
      },
      "source": [
        "# Copy Task\n",
        "\n",
        "## Problem\n",
        "\n",
        "Given a repeating sequence of distinct tokens, continue the pattern. This equates to learning an induction head.\n",
        "\n",
        "## Dataset Generation\n",
        "\n",
        "The dataset consists of sequences of varying length that contain a repeating pattern and cut-off abruptly. The goal is to continue the sequence correctly. There is no semantic meaning behind tokens, so they can be randomly generated at init and frozen.\n",
        "\n",
        "E.g. abcabcabca should be continued with bcabcabc\n",
        "\n",
        "### Base case\n",
        "\n",
        "The most simple case will simply have 64-character strings containing repeating character sequences of 4 to 15 distinct characters, so we see 8 to 4 repetitions. To start we can use 32 distinct tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc3Zj_wEA4T8",
        "outputId": "302e1bb8-74de-4a2d-e41a-fc163838d6fc"
      },
      "outputs": [],
      "source": [
        "# # create dataset; we need to add masking on the loss function!\n",
        "# key_d1 = jax.random.PRNGKey(0)\n",
        "\n",
        "# dataset_name = 'copytask'\n",
        "# token_arr = jnp.arange(128, dtype=jnp.uint16)\n",
        "# sample_len = 64\n",
        "# n_data = 2**19*3\n",
        "# patt_min = 4\n",
        "# patt_max = 15\n",
        "# assert sample_len >= patt_max # so there is at least one repetition\n",
        "# assert patt_max >= patt_min\n",
        "# assert n_data % (patt_max-patt_min+1) == 0 # so each pattern length is represented equally\n",
        "\n",
        "# pattern_inds = []\n",
        "# masks = []\n",
        "# for patt_len in range(patt_min, patt_max+1):\n",
        "#   p = jnp.tile(jnp.arange(patt_len), 1+sample_len//patt_len)[:sample_len]\n",
        "#   pattern_inds.append(p)\n",
        "#   m = jnp.ones(sample_len)\n",
        "#   m = m.at[:patt_len].set(0)\n",
        "#   masks.append(m)\n",
        "# pattern_inds = jnp.array(pattern_inds)\n",
        "# masks = jnp.array(masks)\n",
        "\n",
        "# key_gen, key_shuffle = jax.random.split(key_d1)\n",
        "# key_perms = jax.random.split(key_gen, n_data)\n",
        "\n",
        "# tok_permutations = vmap(lambda k : jax.random.permutation(k, token_arr))(key_perms)\n",
        "# pattern_inds_expanded = jnp.tile(pattern_inds, (n_data//len(pattern_inds), 1))\n",
        "# masks_expanded = jnp.tile(masks, (n_data//len(masks), 1))\n",
        "\n",
        "# data = vmap(lambda i : tok_permutations[i][pattern_inds_expanded[i]])(jnp.arange(n_data))\n",
        "# data_shuffled = data[jax.random.permutation(key_shuffle, jnp.arange(n_data))] # shuffle the data\n",
        "# masks_shuffled = masks_expanded[jax.random.permutation(key_shuffle, jnp.arange(n_data))]\n",
        "# print(pattern_inds.shape, pattern_inds_expanded.shape, masks_expanded.shape, data.shape)\n",
        "# data_with_mask = jnp.concatenate([data_shuffled[None, ...], masks_shuffled[None, ...]], axis=0)\n",
        "# print(data_with_mask.shape)\n",
        "# val_data_len = n_data//16\n",
        "# train_ids = np.array(data_with_mask, dtype=np.uint16)[:, :-val_data_len, :].reshape(2, -1)\n",
        "# val_ids = np.array(data_with_mask, dtype=np.uint16)[:, -val_data_len:, :].reshape(2, -1)\n",
        "# try:\n",
        "#   os.mkdir(dataset_name)\n",
        "# except:\n",
        "#   print(f'dataset {dataset_name} already exists')\n",
        "# train_ids.tofile(os.path.join(dataset_name, 'train_with_mask.bin'))\n",
        "# val_ids.tofile(os.path.join(dataset_name, 'val_with_mask.bin'))\n",
        "\n",
        "# sample_len, n_data, val_data_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset copytask already exists\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(64, 1572864, 98304, (2, 94371840), (2, 6291456))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create dataset; we need to add masking on the loss function!\n",
        "key_d1 = jax.random.PRNGKey(0)\n",
        "\n",
        "dataset_name = 'copytask'\n",
        "token_arr = jnp.arange(64, dtype=jnp.uint16)\n",
        "sample_len = 64\n",
        "n_data = 2**19*3\n",
        "patt_min = 4\n",
        "patt_max = 27\n",
        "assert sample_len >= patt_max # so there is at least one repetition\n",
        "assert patt_max >= patt_min\n",
        "assert n_data % (patt_max-patt_min+1) == 0 # so each pattern length is represented equally\n",
        "\n",
        "tok_special = token_arr[:2] # ensure these always map to inds 0\n",
        "tok_ptr = token_arr[2:] # these are exchangeable, no semantic meaning\n",
        "assert len(tok_ptr) >= patt_max\n",
        "\n",
        "sample_inds = []\n",
        "masks = []\n",
        "for patt_len in range(patt_min, patt_max+1):\n",
        "  pattern_inds = jnp.concatenate([jnp.array([1]), jnp.arange(len(tok_special), patt_len+len(tok_special))])\n",
        "  p = jnp.tile(pattern_inds, 1+sample_len//patt_len)[:sample_len]\n",
        "  p = p.at[0].set(0)\n",
        "  sample_inds.append(p)\n",
        "  m = jnp.ones_like(p)\n",
        "  m = m.at[:len(pattern_inds) + 1].set(0)\n",
        "  masks.append(m)\n",
        "sample_inds = jnp.array(sample_inds)\n",
        "masks = jnp.array(masks)\n",
        "\n",
        "key_gen, key_shuffle = jax.random.split(key_d1)\n",
        "key_perms = jax.random.split(key_gen, n_data)\n",
        "\n",
        "tok_permutations = vmap(lambda k : jnp.concatenate([tok_special, jax.random.choice(k, tok_ptr, shape=tok_ptr.shape)]))(key_perms)\n",
        "pattern_inds_expanded = jnp.tile(sample_inds, (n_data//len(sample_inds), 1))\n",
        "masks_expanded = jnp.tile(masks, (n_data//len(masks), 1))\n",
        "\n",
        "data = vmap(lambda i : tok_permutations[i][pattern_inds_expanded[i]])(jnp.arange(n_data))\n",
        "data_shuffled = data[jax.random.permutation(key_shuffle, jnp.arange(n_data))] # shuffle the data\n",
        "masks_shuffled = masks_expanded[jax.random.permutation(key_shuffle, jnp.arange(n_data))]\n",
        "data_with_mask = jnp.concatenate([data_shuffled[None, ...], masks_shuffled[None, ...]], axis=0)\n",
        "val_data_len = n_data//16\n",
        "train_ids = np.array(data_with_mask, dtype=np.uint16)[:, :-val_data_len, :].reshape(2, -1)\n",
        "val_ids = np.array(data_with_mask, dtype=np.uint16)[:, -val_data_len:, :].reshape(2, -1)\n",
        "try:\n",
        "  os.mkdir(dataset_name)\n",
        "except:\n",
        "  print(f'dataset {dataset_name} already exists')\n",
        "train_ids.tofile(os.path.join(dataset_name, 'train_with_mask.bin'))\n",
        "val_ids.tofile(os.path.join(dataset_name, 'val_with_mask.bin'))\n",
        "\n",
        "sample_len, n_data, val_data_len, train_ids.shape, val_ids.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([ 0,  2, 47, 33, 56, 50, 17, 27,  1,  2, 47, 33, 56, 50, 17, 27,  1,\n",
              "         2, 47, 33, 56, 50, 17, 27,  1,  2, 47, 33, 56, 50, 17, 27,  1,  2,\n",
              "        47, 33, 56, 50, 17, 27,  1,  2, 47, 33, 56, 50, 17, 27,  1,  2, 47,\n",
              "        33, 56, 50, 17, 27,  1,  2, 47, 33, 56, 50, 17, 27], dtype=uint16),\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       dtype=uint16))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i = 0\n",
        "train_ids[0][i*sample_len:(i+1)*sample_len], train_ids[1][i*sample_len:(i+1)*sample_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "h4qUaiwA1L1k"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "Yff_V_BvpXzy",
        "outputId": "a6536ddb-6dc9-4b44-ecc7-fa408df64f29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TrainConfig(mesh_axis_names=('dp',), mesh_shape=(4,), input_bin='copytask/train_with_mask.bin', input_val_bin='copytask/val_with_mask.bin', wandb_project='gpt-jax', wandb_entity=None, wandb_run_name=None, wandb_group=None, wandb_job_type=None, wandb_tags=(), wandb_notes=None, wandb_mode='online', wandb_log_code=True, max_iters=20000, warmup_iters=100, lr_decay_iters=20000, eval_interval=1000, eval_iters=25, log_interval=10000, save_every=0, batch_size=64, gradient_accumulation_steps=1, learning_rate=0.003, min_lr=1e-05, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, n_layer=2, embd_dim=128, head_dim=64, block_size=128, vocab_size=64, dropout=0.0, max_seq_len=384, pos_encoding_base=1000, use_mlp=False, off_by_one_attn=False, use_pope=True, seed=1337, use_masked_loss=True, freeze_params=())"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# %%capture\n",
        "config = train.TrainConfig(\n",
        "    input_bin=f\"{dataset_name}/train_with_mask.bin\",\n",
        "    input_val_bin=f\"{dataset_name}/val_with_mask.bin\",\n",
        "    embd_dim = 128,\n",
        "    head_dim = 64,\n",
        "    n_layer = 2,\n",
        "    block_size = 2*sample_len, # should match the task sequence length so tasks are independently trained on\n",
        "    batch_size = 64,\n",
        "    gradient_accumulation_steps = 1,\n",
        "    max_iters = 20_000,\n",
        "    eval_iters = 25, # val_data_len // 64, # number of examples // batch_size\n",
        "    learning_rate = 3e-3,\n",
        "    min_lr = 1e-5,\n",
        "    warmup_iters = 100,\n",
        "    lr_decay_iters = 20_000,\n",
        "    vocab_size = len(token_arr),\n",
        "    use_masked_loss = True,\n",
        "    use_mlp = False,\n",
        "    off_by_one_attn = False,\n",
        "    use_pope = True,\n",
        "    # freeze_params=(\"wte\",),\n",
        "    max_seq_len = 6*sample_len,\n",
        "\n",
        "    pos_encoding_base = 1_000,\n",
        "    \n",
        "    log_interval = 10_000,\n",
        "    eval_interval = 1_000,\n",
        ")\n",
        "display(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[wandb] No credentials found. Falling back to offline mode.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/home/nikola.georgiev2000/modded-nanogpt-jax/wandb/offline-run-20251226_144355-wwhsjylu</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Number of parameters: 0.14M\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No parameter freezing applied\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Loading training data...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Process 0/1 prepared dataset from 1 file(s): 94,371,840 tokens, 0.38 GB on disk.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Process 0/1 prepared loader with 20000 batches.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Loaded 20000 training batches.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Loading validation data...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Process 0/1 prepared dataset from 1 file(s): 6,291,456 tokens, 0.03 GB on disk.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Process 0/1 prepared loader with 25 batches.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Loaded 25 validation batches.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting Ahead-of-Time (AOT) compilation...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 0.14M\n",
            "No parameter freezing applied\n",
            "Loading training data...\n",
            "Process 0/1 prepared dataset from 1 file(s): 94,371,840 tokens, 0.38 GB on disk.\n",
            "Process 0/1 prepared loader with 20000 batches.\n",
            "Loaded 20000 training batches.\n",
            "Loading validation data...\n",
            "Process 0/1 prepared dataset from 1 file(s): 6,291,456 tokens, 0.03 GB on disk.\n",
            "Process 0/1 prepared loader with 25 batches.\n",
            "Loaded 25 validation batches.\n",
            "Starting Ahead-of-Time (AOT) compilation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: AOT compilation finished.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting training...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 0...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AOT compilation finished.\n",
            "Starting training...\n",
            "Running validation for step 0...\n",
            "model/total_params: 139392 | model/attn_params: 131072 | model/mlp_params: 0 | model/embed_params: 8192 | model/vocab_size: 64 | val_loss: 4.269 | step: 0 | lr: 0 | loss: 4.274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 1000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 1000...\n",
            "val_loss: 2.695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 2000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 2000...\n",
            "val_loss: 0.08015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 3000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 3000...\n",
            "val_loss: 0.01112\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 4000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 4000...\n",
            "val_loss: 0.01514\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 5000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 5000...\n",
            "val_loss: 0.009499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 6000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 6000...\n",
            "val_loss: 0.008644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 7000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 7000...\n",
            "val_loss: 0.006718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 8000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 8000...\n",
            "val_loss: 0.00471\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 9000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 9000...\n",
            "val_loss: 0.003792\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 10000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 10000...\n",
            "val_loss: 0.001989 | step: 10000 | lr: 0.001517 | loss: 0.002922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 11000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 11000...\n",
            "val_loss: 0.002485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Cycling dataset...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cycling dataset...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 12000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 12000...\n",
            "val_loss: 0.001941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 13000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 13000...\n",
            "val_loss: 0.0009329\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 14000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 14000...\n",
            "val_loss: 0.0005848\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 15000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 15000...\n",
            "val_loss: 0.0003796\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 16000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 16000...\n",
            "val_loss: 0.0001481\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 17000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 17000...\n",
            "val_loss: 0.0002051\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 18000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 18000...\n",
            "val_loss: 9.619e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 19000...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running validation for step 19000...\n",
            "val_loss: 9.699e-05\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Final validation...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Running validation for step 19999...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Training finished.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 1 file into the W&B run directory; call wandb.save again to sync new files.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Saved checkpoint to logs/wwhsjylu//state_step019999.pkl\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final validation...\n",
            "Running validation for step 19999...\n",
            "step: 19999 | val_loss: 0.0001069\n",
            "Training finished.\n",
            "Saved checkpoint to logs/wwhsjylu//state_step019999.pkl\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▁</td></tr><tr><td>lr</td><td>▁█</td></tr><tr><td>model/attn_params</td><td>▁</td></tr><tr><td>model/embed_params</td><td>▁</td></tr><tr><td>model/mlp_params</td><td>▁</td></tr><tr><td>model/total_params</td><td>▁</td></tr><tr><td>model/vocab_size</td><td>▁</td></tr><tr><td>step</td><td>▁▅█</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.00292</td></tr><tr><td>lr</td><td>0.00152</td></tr><tr><td>model/attn_params</td><td>131072</td></tr><tr><td>model/embed_params</td><td>8192</td></tr><tr><td>model/mlp_params</td><td>0</td></tr><tr><td>model/total_params</td><td>139392</td></tr><tr><td>model/vocab_size</td><td>64</td></tr><tr><td>step</td><td>19999</td></tr><tr><td>val_loss</td><td>0.00011</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "You can sync this run to the cloud by running:<br><code>wandb sync /home/nikola.georgiev2000/modded-nanogpt-jax/wandb/offline-run-20251226_144355-wwhsjylu<code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/offline-run-20251226_144355-wwhsjylu/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "params = train.train_loop(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zQZ9oUxp9F4C",
        "outputId": "530bd668-804e-4a81-f420-af17601e5e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "patt_len 23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/361 [00:00<?, ?it/s]/home/nikola.georgiev2000/.local/lib/python3.10/site-packages/jax/_src/ops/scatter.py:108: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=uint16 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\n",
            "100%|██████████| 361/361 [00:06<00:00, 56.45it/s]\n"
          ]
        }
      ],
      "source": [
        "test_input = jnp.array([0]+[3,9,8,3,4,4,4,8,5,8,6,6,8,8,2,7,9,7,9,3,6,6], dtype=jnp.uint16)\n",
        "patt_len = len(test_input)\n",
        "print('patt_len', patt_len)\n",
        "res = jnp.zeros(config.max_seq_len, dtype=jnp.uint16)\n",
        "res = res.at[:patt_len].set(test_input).at[patt_len].set(1)\n",
        "\n",
        "rope_params = model.precompute_pope(config.get_model_config(), None) if config.use_pope else model.precompute_rope(config.get_model_config(), None)\n",
        "for i in tqdm(range(patt_len, len(res))):\n",
        "  preds = model.gpt_forward(params, rope_params, res[None,:], config.get_model_config())\n",
        "  new_ind = jnp.argmax(preds[0][i])\n",
        "  res = res.at[i+1].set(new_ind) # if ((i+1) % patt_len > 0) else 1)\n",
        "\n",
        "preds, attn_weights = model.gpt_forward(params, rope_params, res[None,:], config.get_model_config(), return_attn_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 2, 384, 384)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "attn_weights[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WnBGTyqv9TVx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BarContainer object of 64 artists>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.bar(jnp.arange(preds.shape[-1]), preds[0][44])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing input:\n",
            " 0: [0 3 9 8 3 4 4 4 8 5 8 6 6 8 8 2 7 9 7 9 3 6 6]\n",
            "Predicted output:\n",
            " 1: [1 3 9 8 3 4 4 4 8 5 8 6 6 8 8 2 7 9 7 9 3 6 6]\n",
            " 2: [1 3 9 8 3 4 4 4 8 5 8 6 6 8 8 2 7 9 7 9 3 6 6]\n",
            " 3: [1 3 9 8 3 4 4 4 8 5 8 6 6 8 8 2 7 9 7 9 3 6 6]\n",
            " 4: [1 3 9 8 3 4 4 4 8 5 8 6 6 8 8 2 7 9 7 9 3 6 6]\n",
            " 5: [1 3 9 8 3 4 4 4 8 5 8 6 8 2 7 9 7 9 3 6 6 1 3]\n",
            " 6: [9 8 3 4 4 4 8 5 8 6 8 2 7 9 7 9 3 6 6 1 3 9 8]\n",
            " 7: [3 4 4 4 8 5 8 6 8 2 7 9 7 9 3 6 6 1 3 9 8 3 4]\n",
            " 8: [4 4 8 5 8 6 8 2 7 9 7 9 3 6 6 1 3 9 8 3 4 4 8]\n",
            " 9: [5 8 6 8 2 7 9 3 6 1 3 9 8 3 4 4 8 5 8 6 8 2 7]\n",
            "10: [9 3 6 1 3 9 8 3 4 4 8 5 8 6 8 2 7 9 3 6 1 3 9]\n",
            "11: [8 3 4 8 5 8 6 8 2 7 9 3 6 1 3 9 8 3 4 8 5 8 6]\n",
            "12: [8 2 7 9 3 6 1 3 9 8 3 4 8 5 8 6 8 2 7 9 3 6 1]\n",
            "13: [3 9 8 3 4 8 5 8 6 8 2 7 9 3 6 1 3 9 8 3 4 8 5]\n",
            "14: [8 6 8 2 7 9 3 6 1 3 9 8 3 4 8 5 8 6 8 2 7 9 3]\n",
            "15: [6 1 3 9 8 3 4 8 5 8 6 8 2 7 9 3 6 1 3 9 8 3 4]\n"
          ]
        }
      ],
      "source": [
        "res = jnp.array(res)\n",
        "\n",
        "print(\"Testing input:\")\n",
        "print(f'{0:2d}: {jnp.array(test_input[:patt_len])}')\n",
        "\n",
        "print(\"Predicted output:\")\n",
        "for i, l in enumerate(res[patt_len: len(res) - (len(res)%patt_len)].reshape(-1, patt_len)):\n",
        "    # format so index strings are aligned\n",
        "    print(f'{i+1:2d}: {l}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### QUESTION\n",
        "\n",
        "What's the better way to encode the Copy Task for generalizing to longer sequences?\n",
        "\n",
        "1. End all patterns with the special token (0). A possible algorithm to learn is to check if there is a 0 in context and if not, record the previous token. If yes, just query and copy based on context. If we don't use off-by-one attention, the first token will likely be a sink token, which might hurt performance.\n",
        "2. Start patterns with special tokens, 0 for the input and 1 for outputs (copied from the input). This will likely result in a sink token at the first position.\n",
        "\n",
        "\n",
        "### TODO\n",
        "\n",
        "Add a better long-context Copy Task dataset with many examples to properly test generalization.\n",
        "Do a sweep over learning rates, embedding sizes, number of attention heads, number of layers.\n",
        "\n",
        "Should I try a different position encoding? Seems like the model struggles to learn the n-grams need copying."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tqsr1PG7-jCx",
        "outputId": "a368c05e-7698-4943-aa28-a452d80f8fab"
      },
      "source": [
        "# Path Graph\n",
        "\n",
        "## Problem\n",
        "\n",
        "Given a 'goal' token, identify which unique 'path' in context it comes from and return all tokens in the path up to and including the goal.\n",
        "\n",
        "Concretely, suppose your context has two paths: A,B,C and P,Q,R,S. Given a goal R, we would return P,Q,R. This tests the model's ability to build pointers between tokens based on context.\n",
        "\n",
        "## Dataset Generation\n",
        "\n",
        "The dataset consists of a list of sequences. Sequences can be one of three types:\n",
        "- context\n",
        "- input\n",
        "- output\n",
        "\n",
        "No nesting of sequences is allowed.\n",
        "\n",
        "#### Tokenization\n",
        "\n",
        "Overall will use 128 tokens.\n",
        "\n",
        "Special Tokens (with index):\n",
        "- Context Start: 0\n",
        "- Input Start: 1\n",
        "- Output start: 2\n",
        "\n",
        "Each special token implicitly ends the previous sequence and starts a new one. These embeddings can be learned.\n",
        "\n",
        "All remaining tokens are exchangeable, i.e. only act as pointers and contain no semantic meaning. Their embeddings will be fixed and initialized randomly.\n",
        "\n",
        "1. Minimal example\n",
        "- 0, 3,4,5,6, 1, 5, 2, 3,4,5\n",
        "\n",
        "2. Two contexts\n",
        "- 0, 3,4,5,6, 0, 7,8,9, 1, 4, 2, 3,4\n",
        "\n",
        "3. Stream of problems (context grows and problems arrive independently)\n",
        "- 0, 3,4,5,6, 0, 7,8,9, 1, 5, 2, 3,4,5, 0, 10,11,12,13,14, 1, 4, 2, 3,4, 1, 12, 2, 10,11,12\n",
        "\n",
        "For position encoding we'll use RoPE which works well with QK-norm attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fjCOwdxKGfhS"
      },
      "outputs": [],
      "source": [
        "# TBD, need to add masking to loss_fn()!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
